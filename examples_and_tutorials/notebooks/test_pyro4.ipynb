{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pyro4 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pyro4\n",
    "Pyro4.config.SERIALIZER = \"pickle\"\n",
    "Pyro4.config.SERIALIZERS_ACCEPTED.add(\"pickle\")\n",
    "bot = Pyro4.Proxy(\"PYRONAME:remotelocobot@172.17.0.2\")\n",
    "bot.test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import String\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from copy import deepcopy as copy\n",
    "from IPython import embed\n",
    "import sys\n",
    "import time\n",
    "import ray\n",
    "from scipy.spatial.transform import Rotation\n",
    "from pycocotools.coco import COCO\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "def transform_pose(XYZ, current_pose):\n",
    "    \"\"\"\n",
    "    Transforms the point cloud into geocentric frame to account for\n",
    "    camera position\n",
    "    Input:\n",
    "        XYZ                     : ...x3\n",
    "    current_pose            : camera position (x, y, theta (radians))\n",
    "    Output:\n",
    "        XYZ : ...x3\n",
    "    \"\"\"\n",
    "    R = Rotation.from_euler(\"Z\", current_pose[2]).as_matrix()\n",
    "    XYZ = np.matmul(XYZ.reshape(-1, 3), R.T).reshape((-1, 3))\n",
    "    XYZ[:, 0] = XYZ[:, 0] + current_pose[0]\n",
    "    XYZ[:, 1] = XYZ[:, 1] + current_pose[1]\n",
    "    return XYZ\n",
    "\n",
    "def propogate_label(\n",
    "    rgb_imgs: np.ndarray, \n",
    "    depth_imgs: np.ndarray, \n",
    "    label_maps: np.ndarray, \n",
    "    base_pose_data: np.ndarray,\n",
    "    src_index: int, \n",
    "    propagation_step: int,\n",
    "\n",
    "    # root_path: str,\n",
    "    # src_img_indx: int,\n",
    "    # src_label: np.ndarray,\n",
    "    # propogation_step: int,\n",
    "    # base_pose_data: np.ndarray,\n",
    "    # out_dir: str,\n",
    "):\n",
    "    \"\"\"Take the label for src_img_indx and propogate it to [src_img_indx - propogation_step, src_img_indx + propogation_step]\n",
    "    Args:\n",
    "        root_path (str): root path where images are stored\n",
    "        src_img_indx (int): source image index\n",
    "        src_label (np.ndarray): array with labeled images are stored (hwc format)\n",
    "        propogation_step (int): number of steps to progate the label\n",
    "        base_pose_data(np.ndarray): (x,y,theta)\n",
    "        out_dir (str): path to store labeled propogation image\n",
    "    \"\"\"\n",
    "    ### load the inputs ###\n",
    "    # load img\n",
    "    src_img = rgb_imgs[src_index]\n",
    "    # load depth in mm\n",
    "    src_depth = depth_imgs[src_index]\n",
    "    # load robot pose for img index\n",
    "    src_pose = base_pose_data[src_index]\n",
    "\n",
    "    ### data needed to convert depth img to pointcloud ###\n",
    "    # values extracted from pyrobot habitat agent\n",
    "    intrinsic_mat = np.array([[256, 0, 256], [0, 256, 256], [0, 0, 1]])\n",
    "    rot = np.array([[0.0, 0.0, 1.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]])\n",
    "    trans = np.array([0, 0, 0.6])\n",
    "    # precompute some values necessary to depth to point cloud\n",
    "    intrinsic_mat_inv = np.linalg.inv(intrinsic_mat)\n",
    "    height, width, channels = src_img.shape\n",
    "    img_resolution = (height, width)\n",
    "    img_pixs = np.mgrid[0 : img_resolution[0] : 1, 0 : img_resolution[1] : 1]\n",
    "    img_pixs = img_pixs.reshape(2, -1)\n",
    "    img_pixs[[0, 1], :] = img_pixs[[1, 0], :]\n",
    "    uv_one = np.concatenate((img_pixs, np.ones((1, img_pixs.shape[1]))))\n",
    "    uv_one_in_cam = np.dot(intrinsic_mat_inv, uv_one)\n",
    "\n",
    "    ### calculate point cloud in different frmaes ###\n",
    "    # point cloud in camera frmae\n",
    "    depth = (src_depth.astype(np.float32) / 1000.0).reshape(-1)\n",
    "    pts_in_cam = np.multiply(uv_one_in_cam, depth)\n",
    "    pts_in_cam = np.concatenate((pts_in_cam, np.ones((1, pts_in_cam.shape[1]))), axis=0)\n",
    "    # point cloud in robot base frame\n",
    "    pts_in_base = pts_in_cam[:3, :].T\n",
    "    pts_in_base = np.dot(pts_in_base, rot.T)\n",
    "    pts_in_base = pts_in_base + trans.reshape(-1)\n",
    "    # point cloud in world frame (pyrobot)\n",
    "    pts_in_world = transform_pose(pts_in_base, src_pose)\n",
    "\n",
    "    ### figure out unique label values in provided gt label which is greater than 0 ###\n",
    "    unique_pix_value = np.unique(label_maps.reshape(-1), axis=0)\n",
    "    unique_pix_value = [i for i in unique_pix_value if np.linalg.norm(i) > 0]\n",
    "\n",
    "    ### for each unique label, figure out points in wolrd frmae ###\n",
    "    # first figure out pixel index\n",
    "    indx = [zip(*np.where(label_maps == i)) for i in unique_pix_value]\n",
    "    # convert pix index to index in point cloud\n",
    "    # refer this https://www.codepile.net/pile/bZqJbyNz\n",
    "    indx = [[i[0] * width + i[1] for i in j] for j in indx]\n",
    "    # take out points in world space correspoinding to each unique label\n",
    "    req_pts_in_world_list = [pts_in_world[indx[i]] for i in range(len(indx))]\n",
    "\n",
    "    # images in which in which we want to label propagation based on the provided gt seg label\n",
    "    image_range = [max(src_index - propagation_step, 0), src_index + propagation_step]\n",
    "    # param usful to search nearest point cloud in a region\n",
    "    kernal_size = 3\n",
    "\n",
    "    annot_imgs = {}\n",
    "    for img_indx in range(image_range[0], image_range[1]):\n",
    "        print(\"img_index = {}\".format(img_indx))\n",
    "\n",
    "        ### create point cloud in wolrd frame for img_indx ###\n",
    "        # get the robot pose value\n",
    "        base_pose = base_pose_data[img_indx]\n",
    "        # get the depth\n",
    "        try:\n",
    "            cur_depth = depth_imgs[img_indx]\n",
    "        except:\n",
    "            return\n",
    "        # convert depth to point cloud in camera frmae\n",
    "        cur_depth = (cur_depth.astype(np.float32) / 1000.0).reshape(-1)\n",
    "        cur_pts_in_cam = np.multiply(uv_one_in_cam, cur_depth)\n",
    "        cur_pts_in_cam = np.concatenate(\n",
    "            (cur_pts_in_cam, np.ones((1, cur_pts_in_cam.shape[1]))), axis=0\n",
    "        )\n",
    "        # convert point cloud in camera fromae to base frame\n",
    "        cur_pts_in_base = cur_pts_in_cam[:3, :].T\n",
    "        cur_pts_in_base = np.dot(cur_pts_in_base, rot.T)\n",
    "        cur_pts_in_base = cur_pts_in_base + trans.reshape(-1)\n",
    "        # convert point cloud from base frame to world frmae\n",
    "        cur_pts_in_world = transform_pose(cur_pts_in_base, base_pose)\n",
    "\n",
    "        ### generate label for new img indx ###\n",
    "        # crete annotation files with all zeros\n",
    "        annot_img = np.zeros((height, width))\n",
    "        # do label prpogation for each unique label in provided gt seg label\n",
    "        for i, (req_pts_in_world, pix_color) in enumerate(\n",
    "            zip(req_pts_in_world_list, unique_pix_value)\n",
    "        ):\n",
    "            # convert point cloud for label from world pose to current (img_indx) base pose\n",
    "            pts_in_cur_base = copy(req_pts_in_world)\n",
    "            pts_in_cur_base = transform_pose(pts_in_cur_base, (-base_pose[0], -base_pose[1], 0))\n",
    "            pts_in_cur_base = transform_pose(pts_in_cur_base, (0.0, 0.0, -base_pose[2]))\n",
    "\n",
    "            # conver point from current base to current camera frame\n",
    "            pts_in_cur_cam = pts_in_cur_base - trans.reshape(-1)\n",
    "            pts_in_cur_cam = np.dot(pts_in_cur_cam, rot)\n",
    "\n",
    "            # conver pts in current camera frame into 2D pix values\n",
    "            pts_in_cur_img = np.matmul(intrinsic_mat, pts_in_cur_cam.T).T\n",
    "            pts_in_cur_img /= pts_in_cur_img[:, 2].reshape([-1, 1])\n",
    "\n",
    "            # filter out index which fall beyond the shape of img size\n",
    "            filtered_img_indx = np.logical_and(\n",
    "                np.logical_and(0 <= pts_in_cur_img[:, 0], pts_in_cur_img[:, 0] < height),\n",
    "                np.logical_and(0 <= pts_in_cur_img[:, 1], pts_in_cur_img[:, 1] < width),\n",
    "            )\n",
    "\n",
    "            # only consider depth matching for these points\n",
    "            # filter out point based on projected depth value wold frmae, this helps us get rid of pixels for which view to the object is blocked by any other object, as in that was projected 3D point in wolrd frmae for the current pix wont match with 3D point in the gt provide label\n",
    "            dist_thr = 5e-2  # this is in meter\n",
    "            for pixel_index in range(len(filtered_img_indx)):\n",
    "                if filtered_img_indx[pixel_index]:\n",
    "                    # search in the region\n",
    "                    gt_pix_depth_in_world = req_pts_in_world[pixel_index]\n",
    "                    p, q = np.meshgrid(\n",
    "                        range(\n",
    "                            int(pts_in_cur_img[pixel_index][1] - kernal_size),\n",
    "                            int(pts_in_cur_img[pixel_index][1] + kernal_size),\n",
    "                        ),\n",
    "                        range(\n",
    "                            int(pts_in_cur_img[pixel_index][0] - kernal_size),\n",
    "                            int(pts_in_cur_img[pixel_index][0] + kernal_size),\n",
    "                        ),\n",
    "                    )\n",
    "                    loc = p * width + q\n",
    "                    loc = loc.reshape(-1).astype(np.int)\n",
    "                    loc = np.clip(loc, 0, width * height - 1).astype(np.int)\n",
    "\n",
    "                    if (\n",
    "                        min(np.linalg.norm(cur_pts_in_world[loc] - gt_pix_depth_in_world, axis=1))\n",
    "                        > dist_thr\n",
    "                    ):\n",
    "                        filtered_img_indx[pixel_index] = False\n",
    "\n",
    "            # take out filtered pix values\n",
    "            pts_in_cur_img = pts_in_cur_img[filtered_img_indx]\n",
    "\n",
    "            # step to take care of quantization erros\n",
    "            pts_in_cur_img = np.concatenate(\n",
    "                (\n",
    "                    np.concatenate(\n",
    "                        (\n",
    "                            np.ceil(pts_in_cur_img[:, 0]).reshape(-1, 1),\n",
    "                            np.ceil(pts_in_cur_img[:, 1]).reshape(-1, 1),\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    ),\n",
    "                    np.concatenate(\n",
    "                        (\n",
    "                            np.floor(pts_in_cur_img[:, 0]).reshape(-1, 1),\n",
    "                            np.floor(pts_in_cur_img[:, 1]).reshape(-1, 1),\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    ),\n",
    "                    np.concatenate(\n",
    "                        (\n",
    "                            np.ceil(pts_in_cur_img[:, 0]).reshape(-1, 1),\n",
    "                            np.floor(pts_in_cur_img[:, 1]).reshape(-1, 1),\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    ),\n",
    "                    np.concatenate(\n",
    "                        (\n",
    "                            np.floor(pts_in_cur_img[:, 0]).reshape(-1, 1),\n",
    "                            np.ceil(pts_in_cur_img[:, 1]).reshape(-1, 1),\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            pts_in_cur_img = pts_in_cur_img[:, :2].astype(int)\n",
    "\n",
    "            # filter out index which fall beyonf the shape of img size, had to perform this step again to take care if any out of the image size point is introduced by the above quantization step\n",
    "            pts_in_cur_img = pts_in_cur_img[\n",
    "                np.logical_and(\n",
    "                    np.logical_and(0 <= pts_in_cur_img[:, 0], pts_in_cur_img[:, 0] < height),\n",
    "                    np.logical_and(0 <= pts_in_cur_img[:, 1], pts_in_cur_img[:, 1] < width),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # number of pointf for the label found in cur img\n",
    "            print(\"pts in cam = {}\".format(len(pts_in_cur_cam)))\n",
    "\n",
    "            # assign label to correspoinding pix values\n",
    "            annot_img[pts_in_cur_img[:, 1], pts_in_cur_img[:, 0]] = pix_color\n",
    "\n",
    "        # store the annotation file\n",
    "        annot_imgs[img_indx] = annot_img.astype(np.uint32)\n",
    "    return annot_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(rgb_obs, semantic_obs, depth_obs):\n",
    "    rgb_img = Image.fromarray(rgb_obs)\n",
    "\n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGBA\")\n",
    "\n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "\n",
    "    arr = [rgb_img, semantic_img, depth_img]\n",
    "    titles = ['rgb', 'semantic', 'depth']\n",
    "    plt.figure(figsize=(12 ,8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i+1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d3_40_colors_rgb: np.ndarray = np.array(\n",
    "    [\n",
    "        [31, 119, 180],\n",
    "        [174, 199, 232],\n",
    "        [255, 127, 14],\n",
    "        [255, 187, 120],\n",
    "        [44, 160, 44],\n",
    "        [152, 223, 138],\n",
    "        [214, 39, 40],\n",
    "        [255, 152, 150],\n",
    "        [148, 103, 189],\n",
    "        [197, 176, 213],\n",
    "        [140, 86, 75],\n",
    "        [196, 156, 148],\n",
    "        [227, 119, 194],\n",
    "        [247, 182, 210],\n",
    "        [127, 127, 127],\n",
    "        [199, 199, 199],\n",
    "        [188, 189, 34],\n",
    "        [219, 219, 141],\n",
    "        [23, 190, 207],\n",
    "        [158, 218, 229],\n",
    "        [57, 59, 121],\n",
    "        [82, 84, 163],\n",
    "        [107, 110, 207],\n",
    "        [156, 158, 222],\n",
    "        [99, 121, 57],\n",
    "        [140, 162, 82],\n",
    "        [181, 207, 107],\n",
    "        [206, 219, 156],\n",
    "        [140, 109, 49],\n",
    "        [189, 158, 57],\n",
    "        [231, 186, 82],\n",
    "        [231, 203, 148],\n",
    "        [132, 60, 57],\n",
    "        [173, 73, 74],\n",
    "        [214, 97, 107],\n",
    "        [231, 150, 156],\n",
    "        [123, 65, 115],\n",
    "        [165, 81, 148],\n",
    "        [206, 109, 189],\n",
    "        [222, 158, 214],\n",
    "    ],\n",
    "    dtype=np.uint8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb, d, segm = bot.get_rgbd_segm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-88f3c07cb44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdepth_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdepth0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msegm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropogate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# print(segm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-894e0258fc9f>\u001b[0m in \u001b[0;36mpropogate_label\u001b[0;34m(rgb_imgs, depth_imgs, label_maps, base_pose_data, src_index, propagation_step)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_depth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpts_in_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muv_one_in_cam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mpts_in_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_in_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_in_cam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m# point cloud in robot base frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mpts_in_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts_in_cam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# segm = np.load('hi.npy', allow_pickle=True)\n",
    "\n",
    "# These are the inputs from the dashboard that are to be used in the label prop script\n",
    "rgb0 = np.load(\"rgb0.npy\", allow_pickle=True)\n",
    "rgb1 = np.load(\"rgb1.npy\", allow_pickle=True)\n",
    "depth0 = np.load(\"depth0.npy\", allow_pickle=True)\n",
    "depth1 = np.load(\"depth1.npy\", allow_pickle=True)\n",
    "labels = np.load(\"label_maps.npy\", allow_pickle=True)\n",
    "pose = np.load(\"base_pose.npy\", allow_pickle=True)\n",
    "\n",
    "rgb_imgs = np.array([rgb0, rgb1])\n",
    "depth_imgs = np.array([depth0, depth0])\n",
    "\n",
    "segm = propogate_label(rgb_imgs, depth_imgs, labels, pose, 1, 1)\n",
    "\n",
    "# print(segm)\n",
    "print(np.unique(segm[0], return_counts=True))\n",
    "print(np.unique(labels[0], return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(2, 512, 512)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "print(segm[0].shape)\n",
    "print(labels.shape)\n",
    "print(depth0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Too many dimensions: 3 > 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-af1d8be33122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdepth_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORM_MINMAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# showing input labels and depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-08f58d99c013>\u001b[0m in \u001b[0;36mdisplay_sample\u001b[0;34m(rgb_obs, semantic_obs, depth_obs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msemantic_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdepth_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_obs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/droidlet_env/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2782\u001b[0m         \u001b[0mndmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Too many dimensions: {ndim} > {ndmax}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Too many dimensions: 3 > 2."
     ]
    }
   ],
   "source": [
    "depth_img = cv2.normalize(depth0, None, 0, 255, cv2.NORM_MINMAX)\n",
    "display_sample(rgb, labels[0], depth_img) # showing input labels and depth\n",
    "display_sample(rgb, segm[0], d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) save dashboard inputs to disk\n",
    "# 2) define label prop function\n",
    "# 3) load inputs from disk and call label prop\n",
    "# 4) visualize output with display_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
